# -*- coding: utf-8 -*-
"""Oversampling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u3SMhkG82SOJwOwPF2YP-vKtJ4LnV6lp
"""

import tensorflow as tf
import pandas as pd
import numpy as np

dataset_path = '../dataset/train_baseline.csv'
dataset = pd.read_csv(dataset_path, encoding='unicode_escape')

dataset.drop(['Unnamed: 0'], inplace=True, axis=1)
dataset["Tag"].value_counts()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(16, 8))
sns.set_style('darkgrid')
sns.histplot(dataset['Tag'])
plt.tight_layout(pad=2)
plt.show()

dataset_fillna = dataset.fillna(method='ffill', axis=0)
dataset_group = dataset_fillna.groupby(['Sentence #'], as_index=False)['Word', 'Tag', 'Word_idx', 'Tag_idx'].agg(
    lambda x: list(x))

dataset_group

"""Membuat list data kandidat yang akan digunakan saat melakukan oversampling. Data-data ini diambil secara acak dari dataset awal, sehingga metode ini mengandung unsur random oversampling pada sisi pengambilan kandidat data yang akan dibangkitkan"""

geoList = ["States", "City", "Province", "Islands", "Republic"]
orgList = ["Agency", "Council", "Association", "Company", "Community", "Committee", "Department", "Organization"]
perList = ["Pinochet", "Montiglio", "George", "Laurent", "Levy", "Abbas", "Felipe", "Sharon", "Ariel", "Lavrov", "Bush",
           "Silva", "Putin", "Chavez", "Rosales", "Howard", "Bashar", "Nahles", "Johnson", "Morris", "Davis", "Robert",
           "Karzai", "Jalal"]

import random


def generateRandomEntity(entityType):
    if (entityType == 'GEO'):
        return random.choice(geoList)
    elif (entityType == 'ORG'):
        return random.choice(orgList)
    else:
        return random.choice(perList)


"""Load dictionary dari hasil preprocessing untuk memetakan token2idx dan sebaliknya serta tag2idx dan sebaliknya."""

# importing the module
import json


# Opening JSON file
def loadJSON(filename):
    data = {}
    with open(filename) as json_file:
        data = json.load(json_file)
    return data


idx2tag = loadJSON('../output/idx2tag.json')
tag2idx = loadJSON('../output/tag2idx.json')
token2idx = loadJSON('../output/token2idx.json')
idx2token = loadJSON('../output/idx2token.json')
print(idx2tag)


def oversample(data):
    last = False
    for sentenceIdx in range(0, data.shape[0]):
        for tagIdx in range(0, len(data.loc[sentenceIdx, 'Tag'])):
            if (tagIdx == len(data.loc[sentenceIdx, 'Tag']) - 1):
                last = True
            # Oversampling process
            if ((data.loc[sentenceIdx, 'Tag'][tagIdx] == 'B-geo' and last == True) or (
                    data.loc[sentenceIdx, 'Tag'][tagIdx] == 'B-geo' and data.loc[sentenceIdx, 'Tag'][
                tagIdx + 1] == 'O')):
                random_word = generateRandomEntity('GEO')
                # print(random_word)
                data.loc[sentenceIdx, 'Word'].insert(tagIdx + 1, random_word)
                data.loc[sentenceIdx, 'Word_idx'].insert(tagIdx + 1, token2idx.get(random_word))
                data.loc[sentenceIdx, 'Tag'].insert(tagIdx + 1, 'I-geo')
                data.loc[sentenceIdx, 'Tag_idx'].insert(tagIdx + 1, tag2idx.get('I-geo'))
            elif ((data.loc[sentenceIdx, 'Tag'][tagIdx] == 'B-org' and last == True) or (
                    data.loc[sentenceIdx, 'Tag'][tagIdx] == 'B-org' and data.loc[sentenceIdx, 'Tag'][
                tagIdx + 1] == 'O')):
                random_word = generateRandomEntity('ORG')
                # print(random_word)
                data.loc[sentenceIdx, 'Word'].insert(tagIdx + 1, random_word)
                data.loc[sentenceIdx, 'Word_idx'].insert(tagIdx + 1, token2idx.get(random_word))
                data.loc[sentenceIdx, 'Tag'].insert(tagIdx + 1, 'I-org')
                data.loc[sentenceIdx, 'Tag_idx'].insert(tagIdx + 1, tag2idx.get('I-org'))
            elif ((data.loc[sentenceIdx, 'Tag'][tagIdx] == 'B-per' and last == True) or (
                    data.loc[sentenceIdx, 'Tag'][tagIdx] == 'B-per' and data.loc[sentenceIdx, 'Tag'][
                tagIdx + 1] == 'O')):
                random_word = generateRandomEntity('PER')
                # print(random_word)
                data.loc[sentenceIdx, 'Word'].insert(tagIdx + 1, random_word)
                data.loc[sentenceIdx, 'Word_idx'].insert(tagIdx + 1, token2idx.get(random_word))
                data.loc[sentenceIdx, 'Tag'].insert(tagIdx + 1, 'I-per')
                data.loc[sentenceIdx, 'Tag_idx'].insert(tagIdx + 1, tag2idx.get('I-per'))

    return data


def nerOversample(dataset):
    last = False
    for sentence in range(0, dataset.shape[0]):
        for tag in range(0, len(dataset.loc[sentence, 'Tag'])):
            if (tag == len(dataset.loc[sentence, 'Tag']) - 1):
                last = True
            if ((dataset.loc[sentence, 'Tag'][tag] == 'B-geo' and last == True) or (
                    dataset.loc[sentence, 'Tag'][tag] == 'B-geo' and dataset.loc[sentence, 'Tag'][tag + 1] == 'O')):
                if (last == False):
                    print(sentence, tag)
                    print(dataset.loc[sentence, 'Word'][tag], dataset.loc[sentence, 'Word'][tag + 1],
                          dataset.loc[sentence, 'Tag'][tag], dataset.loc[sentence, 'Tag'][tag + 1])
                random_word = generateRandomEntity('GEO')
                dataset.loc[sentence, 'Word'].insert(tag + 1, random_word)
                dataset.loc[sentence, 'Word_idx'].insert(tag + 1, token2idx.get(random_word))
                dataset.loc[sentence, 'Tag'].insert(tag + 1, 'I-geo')
                dataset.loc[sentence, 'Tag_idx'].insert(tag + 1, tag2idx.get('I-geo'))
            elif ((dataset.loc[sentence, 'Tag'][tag] == 'B-org' and last == True) or (
                    dataset.loc[sentence, 'Tag'][tag] == 'B-org' and dataset.loc[sentence, 'Tag'][tag + 1] == 'O')):
                if (last == False):
                    print(sentence, tag)
                    print(dataset.loc[sentence, 'Word'][tag], dataset.loc[sentence, 'Word'][tag + 1],
                          dataset.loc[sentence, 'Tag'][tag], dataset.loc[sentence, 'Tag'][tag + 1])
                random_word = generateRandomEntity('ORG')
                dataset.loc[sentence, 'Word'].insert(tag + 1, random_word)
                dataset.loc[sentence, 'Word_idx'].insert(tag + 1, token2idx.get(random_word))
                dataset.loc[sentence, 'Tag'].insert(tag + 1, 'I-org')
                dataset.loc[sentence, 'Tag_idx'].insert(tag + 1, tag2idx.get('I-org'))
            elif ((dataset.loc[sentence, 'Tag'][tag] == 'B-per' and last == True) or (
                    dataset.loc[sentence, 'Tag'][tag] == 'B-per' and dataset.loc[sentence, 'Tag'][tag + 1] == 'O')):
                if (last == False):
                    print(sentence, tag)
                    print(dataset.loc[sentence, 'Word'][tag], dataset.loc[sentence, 'Word'][tag + 1],
                          dataset.loc[sentence, 'Tag'][tag], dataset.loc[sentence, 'Tag'][tag + 1])
                random_word = generateRandomEntity('PER')
                dataset.loc[sentence, 'Word'].insert(tag + 1, random_word)
                dataset.loc[sentence, 'Word_idx'].insert(tag + 1, token2idx.get(random_word))
                dataset.loc[sentence, 'Tag'].insert(tag + 1, 'I-per')
                dataset.loc[sentence, 'Tag_idx'].insert(tag + 1, tag2idx.get('I-per'))
    return dataset


"""Backup dataset sebelum melakukan oversampling"""

data_backup = dataset_group.copy()
dataset_untuk_sample = data_backup.copy()

dataset_sampled = oversample(dataset_untuk_sample)

dataset_sampled

"""Kembalikan dataframe untuk melihat hasil oversampling"""


def buildDataFrame(groupedData):
    word_list = []
    tag_list = []
    sentence_list = []
    word_idx = []
    tag_idx = []
    for sentence in range(0, groupedData.shape[0]):
        sentence_array = [groupedData.loc[sentence, 'Sentence #']] * len(groupedData.loc[sentence, 'Word'])
        sentence_list = sentence_list + sentence_array
        word_list = word_list + groupedData.loc[sentence, 'Word']
        tag_list = tag_list + groupedData.loc[sentence, 'Tag']
        word_idx = tag_list + groupedData.loc[sentence, 'Word_idx']
        tag_idx = tag_list + groupedData.loc[sentence, 'Tag_idx']

        # print(sentence_list, word_list, tag_list)
    dataFrame = pd.DataFrame(
        {'Sentence #': sentence_list, 'Word': word_list, 'Tag': tag_list, 'Word_idx': word_idx, 'Tag_idx': tag_idx})
    # return sentence_list, word_list, tag_list
    return dataFrame


train_df_after_sampling = buildDataFrame(dataset_sampled)

train_df_after_sampling["Tag"].value_counts()

train_df_after_sampling.to_csv('../dataset/train_nerOversampled.csv')
