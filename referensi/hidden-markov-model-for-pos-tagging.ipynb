{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt # show graph\n\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom hmmlearn import hmm\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-20T21:46:45.980052Z","iopub.execute_input":"2021-10-20T21:46:45.980554Z","iopub.status.idle":"2021-10-20T21:46:46.593918Z","shell.execute_reply.started":"2021-10-20T21:46:45.980482Z","shell.execute_reply":"2021-10-20T21:46:46.593196Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/name-entity-recognition-ner-dataset/NER dataset.csv\", encoding='latin1')\ndata = data.fillna(method=\"ffill\")\ndata = data.rename(columns={'Sentence #': 'sentence'})\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:46.597406Z","iopub.execute_input":"2021-10-20T21:46:46.597658Z","iopub.status.idle":"2021-10-20T21:46:47.840632Z","shell.execute_reply.started":"2021-10-20T21:46:46.597626Z","shell.execute_reply":"2021-10-20T21:46:47.839703Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"      sentence           Word  POS Tag\n0  Sentence: 1      Thousands  NNS   O\n1  Sentence: 1             of   IN   O\n2  Sentence: 1  demonstrators  NNS   O\n3  Sentence: 1           have  VBP   O\n4  Sentence: 1        marched  VBN   O","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentence: 1</td>\n      <td>Thousands</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sentence: 1</td>\n      <td>of</td>\n      <td>IN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sentence: 1</td>\n      <td>demonstrators</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sentence: 1</td>\n      <td>have</td>\n      <td>VBP</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sentence: 1</td>\n      <td>marched</td>\n      <td>VBN</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Get the numbers of tags & words inside the whole data. We'll need this in the future.","metadata":{}},{"cell_type":"code","source":"tags = list(set(data.POS.values)) #Read POS values\nwords = list(set(data.Word.values))\nlen(tags), len(words)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:47.842255Z","iopub.execute_input":"2021-10-20T21:46:47.842792Z","iopub.status.idle":"2021-10-20T21:46:47.990971Z","shell.execute_reply.started":"2021-10-20T21:46:47.842733Z","shell.execute_reply":"2021-10-20T21:46:47.990091Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(42, 35178)"},"metadata":{}}]},{"cell_type":"markdown","source":"We cannot split data normally with `train_test_split` because doing that makes some parts of a sentence in the training set while some others in the testing set. Instead, we use `GroupShuffleSplit`.","metadata":{}},{"cell_type":"code","source":"y = data.POS\nX = data.drop('POS', axis=1)\n\ngs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\ntrain_ix, test_ix = next(gs.split(X, y, groups=data['sentence']))\n\ndata_train = data.loc[train_ix]\ndata_test = data.loc[test_ix]\n\ndata_train","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:47.994256Z","iopub.execute_input":"2021-10-20T21:46:47.995700Z","iopub.status.idle":"2021-10-20T21:46:49.891110Z","shell.execute_reply.started":"2021-10-20T21:46:47.995654Z","shell.execute_reply":"2021-10-20T21:46:49.890296Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                sentence       Word  POS Tag\n24           Sentence: 2   Families  NNS   O\n25           Sentence: 2         of   IN   O\n26           Sentence: 2   soldiers  NNS   O\n27           Sentence: 2     killed  VBN   O\n28           Sentence: 2         in   IN   O\n...                  ...        ...  ...  ..\n1048570  Sentence: 47959       they  PRP   O\n1048571  Sentence: 47959  responded  VBD   O\n1048572  Sentence: 47959         to   TO   O\n1048573  Sentence: 47959        the   DT   O\n1048574  Sentence: 47959     attack   NN   O\n\n[702936 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>Word</th>\n      <th>POS</th>\n      <th>Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24</th>\n      <td>Sentence: 2</td>\n      <td>Families</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Sentence: 2</td>\n      <td>of</td>\n      <td>IN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Sentence: 2</td>\n      <td>soldiers</td>\n      <td>NNS</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Sentence: 2</td>\n      <td>killed</td>\n      <td>VBN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Sentence: 2</td>\n      <td>in</td>\n      <td>IN</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1048570</th>\n      <td>Sentence: 47959</td>\n      <td>they</td>\n      <td>PRP</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1048571</th>\n      <td>Sentence: 47959</td>\n      <td>responded</td>\n      <td>VBD</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1048572</th>\n      <td>Sentence: 47959</td>\n      <td>to</td>\n      <td>TO</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1048573</th>\n      <td>Sentence: 47959</td>\n      <td>the</td>\n      <td>DT</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1048574</th>\n      <td>Sentence: 47959</td>\n      <td>attack</td>\n      <td>NN</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>702936 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"After checking the data after splitted, it seems to be fine.\nCheck the numbers of tags & words in the training set.","metadata":{"execution":{"iopub.status.busy":"2021-10-20T20:11:20.970165Z","iopub.execute_input":"2021-10-20T20:11:20.970394Z","iopub.status.idle":"2021-10-20T20:11:20.975803Z","shell.execute_reply.started":"2021-10-20T20:11:20.970358Z","shell.execute_reply":"2021-10-20T20:11:20.974565Z"}}},{"cell_type":"code","source":"tags = list(set(data_train.POS.values)) #Read POS values\nwords = list(set(data_train.Word.values))\nlen(tags), len(words)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:49.892689Z","iopub.execute_input":"2021-10-20T21:46:49.893229Z","iopub.status.idle":"2021-10-20T21:46:50.015418Z","shell.execute_reply.started":"2021-10-20T21:46:49.893188Z","shell.execute_reply":"2021-10-20T21:46:50.014596Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(42, 29587)"},"metadata":{}}]},{"cell_type":"markdown","source":"The number of tags is enough but the number of words is not enough (~29k vs ~35k).\nBecause of that we need to randomly add some UNKNOWN words into the training dataset then we recalculate the word list and create map from them to number.","metadata":{}},{"cell_type":"code","source":"dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\ndfupdate.Word = 'UNKNOWN'\ndata_train.update(dfupdate)\nwords = list(set(data_train.Word.values))\n# Convert words and tags into numbers\nword2id = {w: i for i, w in enumerate(words)}\ntag2id = {t: i for i, t in enumerate(tags)}\nid2tag = {i: t for i, t in enumerate(tags)}\nlen(tags), len(words)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:50.016583Z","iopub.execute_input":"2021-10-20T21:46:50.016857Z","iopub.status.idle":"2021-10-20T21:46:50.410317Z","shell.execute_reply.started":"2021-10-20T21:46:50.016820Z","shell.execute_reply":"2021-10-20T21:46:50.409744Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(42, 27554)"},"metadata":{}}]},{"cell_type":"markdown","source":"Hidden Markov Models can be trained by using the Baum-Welch algorithm.\nHowever input of the training is just dataset (Words).\nWe cannot map back the states to the POS tag.\n\nThat's why we have to calculate the model parameters for `hmmlearn.hmm.MultinomialHMM` manually by calculating\n- `startprob_`\n- `transmat_`\n- `emissionprob_`","metadata":{}},{"cell_type":"code","source":"count_tags = dict(data_train.POS.value_counts())\ncount_tags_to_words = data_train.groupby(['POS']).apply(lambda grp: grp.groupby('Word')['POS'].count().to_dict()).to_dict()\ncount_init_tags = dict(data_train.groupby('sentence').first().POS.value_counts())\n\n# TODO use panda solution\ncount_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\nsentences = list(data_train.sentence)\npos = list(data_train.POS)\nfor i in range(len(sentences)) :\n    if (i > 0) and (sentences[i] == sentences[i - 1]):\n        prevtagid = tag2id[pos[i - 1]]\n        nexttagid = tag2id[pos[i]]\n        count_tags_to_next_tags[prevtagid][nexttagid] += 1","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:50.411567Z","iopub.execute_input":"2021-10-20T21:46:50.412372Z","iopub.status.idle":"2021-10-20T21:46:52.901682Z","shell.execute_reply.started":"2021-10-20T21:46:50.412335Z","shell.execute_reply":"2021-10-20T21:46:52.900710Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"mystartprob = np.zeros((len(tags),))\nmytransmat = np.zeros((len(tags), len(tags)))\nmyemissionprob = np.zeros((len(tags), len(words)))\nnum_sentences = sum(count_init_tags.values())\nsum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\nfor tag, tagid in tag2id.items():\n    floatCountTag = float(count_tags.get(tag, 0))\n    mystartprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n    for word, wordid in word2id.items():\n        myemissionprob[tagid][wordid]= count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n    for tag2, tagid2 in tag2id.items():\n        mytransmat[tagid][tagid2]= count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:52.902940Z","iopub.execute_input":"2021-10-20T21:46:52.903251Z","iopub.status.idle":"2021-10-20T21:46:54.236735Z","shell.execute_reply.started":"2021-10-20T21:46:52.903216Z","shell.execute_reply":"2021-10-20T21:46:54.235733Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Initialize a HMM","metadata":{}},{"cell_type":"code","source":"model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi', random_state=42)\nmodel.startprob_ = mystartprob\nmodel.transmat_ = mytransmat\nmodel.emissionprob_ = myemissionprob","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:54.237926Z","iopub.execute_input":"2021-10-20T21:46:54.238163Z","iopub.status.idle":"2021-10-20T21:46:54.243255Z","shell.execute_reply.started":"2021-10-20T21:46:54.238133Z","shell.execute_reply":"2021-10-20T21:46:54.242324Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"As some words may never appear in the training set, we need to transform them into `UNKNOWN` first.\nThen we split `data_test` into `samples` & `lengths` and send them to HMM.","metadata":{}},{"cell_type":"code","source":"data_test.loc[~data_test['Word'].isin(words), 'Word'] = 'UNKNOWN'\nword_test = list(data_test.Word)\nsamples = []\nfor i, val in enumerate(word_test):\n    samples.append([word2id[val]])\n    \n# TODO use panda solution\nlengths = []\ncount = 0\nsentences = list(data_test.sentence)\nfor i in range(len(sentences)) :\n    if (i > 0) and (sentences[i] == sentences[i - 1]):\n        count += 1\n    elif i > 0:\n        lengths.append(count)\n        count = 1\n    else:\n        count = 1","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:54.244442Z","iopub.execute_input":"2021-10-20T21:46:54.244806Z","iopub.status.idle":"2021-10-20T21:46:55.154971Z","shell.execute_reply.started":"2021-10-20T21:46:54.244773Z","shell.execute_reply":"2021-10-20T21:46:55.153984Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# This code is very slow\npos_predict = model.predict(samples, lengths)\npos_predict","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:46:55.156418Z","iopub.execute_input":"2021-10-20T21:46:55.156826Z","iopub.status.idle":"2021-10-20T21:49:56.298549Z","shell.execute_reply.started":"2021-10-20T21:46:55.156781Z","shell.execute_reply":"2021-10-20T21:49:56.297746Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([ 7, 32,  7, ..., 23, 41,  8], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"tags_test = list(data_test.POS)\npos_test = np.zeros((len(tags_test), ), dtype=int)\nfor i, val in enumerate(tags_test):\n    pos_test[i] = tag2id[val]\nlen(pos_predict), len(pos_test), len(samples), len(word_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:49:56.299571Z","iopub.execute_input":"2021-10-20T21:49:56.299781Z","iopub.status.idle":"2021-10-20T21:49:56.482558Z","shell.execute_reply.started":"2021-10-20T21:49:56.299756Z","shell.execute_reply":"2021-10-20T21:49:56.481010Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(345615, 345639, 345639, 345639)"},"metadata":{}}]},{"cell_type":"markdown","source":"Somehow the output of HMM is in wrong size. Only use the shorter length to check the result.","metadata":{}},{"cell_type":"code","source":"def reportTest(y_pred, y_test):\n    print(\"The accuracy is {}\".format(accuracy_score(y_test, y_pred))) \n    print(\"The precision is {}\".format(precision_score(y_test, y_pred, average='weighted'))) \n    print(\"The recall is {}\".format(recall_score(y_test, y_pred, average='weighted'))) \n    print(\"The F1-Score is {}\".format(f1_score(y_test, y_pred, average='weighted')))\n\nmin_length = min(len(pos_predict), len(pos_test))\n\nreportTest(pos_predict[:min_length], pos_test[:min_length])","metadata":{"execution":{"iopub.status.busy":"2021-10-20T21:49:56.484763Z","iopub.execute_input":"2021-10-20T21:49:56.484981Z","iopub.status.idle":"2021-10-20T21:49:57.078323Z","shell.execute_reply.started":"2021-10-20T21:49:56.484954Z","shell.execute_reply":"2021-10-20T21:49:57.077241Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"The accuracy is 0.9656062381551727\nThe precision is 0.9657832270028688\nThe recall is 0.9656062381551727\nThe F1-Score is 0.9655716883723663\n","output_type":"stream"}]}]}