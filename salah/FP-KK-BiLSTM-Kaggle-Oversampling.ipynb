{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/swardiantara/fp-kk-2021/blob/main/FP_KK_NER_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2ulj3IIK5w5"
   },
   "source": [
    "https://www.kaggle.com/alikmondal/named-entity-recognition-using-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "7yO08r9ZHN_H"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nn1ouC9EHrRk",
    "outputId": "51dc2af1-34aa-4c29-ad61-8c376b9facb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 5)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = './dataset/ner_datasetreference.csv'\n",
    "dataset = pd.read_csv(dataset_path, encoding= 'unicode_escape')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "v4Kl3drKICu4",
    "outputId": "53ef9e46-5f35-471d-fa5c-966061252351"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  Class\n",
       "0  Sentence: 1      Thousands  NNS   O      0\n",
       "1          NaN             of   IN   O      0\n",
       "2          NaN  demonstrators  NNS   O      0\n",
       "3          NaN           have  VBP   O      0\n",
       "4          NaN        marched  VBN   O      0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "v522jNLlw0dn"
   },
   "outputs": [],
   "source": [
    "dataset.drop(['POS', 'Class'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrviKqWUbpAj",
    "outputId": "d8f0e090-a3f2-4dcc-b18a-c8566106337a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "MaE0b6DQ94_U",
    "outputId": "1cc3d5cb-dd3c-41eb-d193-e904cdc9d65a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word Tag\n",
       "0  Sentence: 1      Thousands   O\n",
       "1          NaN             of   O\n",
       "2          NaN  demonstrators   O\n",
       "3          NaN           have   O\n",
       "4          NaN        marched   O"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "id": "rK2H5sqoIROY",
    "outputId": "6cdf492a-c624-444d-e6cb-de1c9e17d204"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'O'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-305f514f2258>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'darkgrid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36mdistplot\u001b[1;34m(a, bins, hist, kde, rug, fit, hist_kws, kde_kws, rug_kws, fit_kws, color, vertical, norm_hist, axlabel, label, ax)\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0mkws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"edgecolor\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[0mkws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"color\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkws\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    795\u001b[0m               dtype='datetime64[ns]')\n\u001b[0;32m    796\u001b[0m         \"\"\"\n\u001b[1;32m--> 797\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'O'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (16,8))\n",
    "sns.set_style('darkgrid')\n",
    "sns.distplot(dataset['Tag'])\n",
    "plt.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "QQQ1H4ijWNAb"
   },
   "outputs": [],
   "source": [
    "def computeRatioAwal(dataset):\n",
    "  negative = dataset.apply(lambda x : True if x['Tag'] == \"O\" else False, axis = 1)\n",
    "  num_negative = len(negative[negative == True].index)\n",
    "  num_positive = len(negative[negative == False].index)\n",
    "  return num_negative / num_positive \n",
    "# Count number of True in the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Zm0JR5AWc4v",
    "outputId": "cff4026a-2c6e-4d4b-85be-f9b8ac6e328f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.526386874716027\n"
     ]
    }
   ],
   "source": [
    "print(computeRatioAwal(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMJ94c0ggpd_",
    "outputId": "d5988a4a-44df-4136-9e5f-4c5b0ab48619"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sward\\.conda\\envs\\Data Mining\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dataset_fillna = dataset.fillna(method='ffill', axis=0)\n",
    "dataset_group = dataset_fillna.groupby(['Sentence #'], as_index=False)['Word', 'Tag'].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "lO6uvN1Ug1AN",
    "outputId": "37c57113-9e39-4384-e827-d752db3a86a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47954</th>\n",
       "      <td>Sentence: 9995</td>\n",
       "      <td>[Opposition, leader, Mir, Hossein, Mousavi, ha...</td>\n",
       "      <td>[O, O, O, B-per, I-per, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47955</th>\n",
       "      <td>Sentence: 9996</td>\n",
       "      <td>[On, Thursday, ,, Iranian, state, media, publi...</td>\n",
       "      <td>[O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47956</th>\n",
       "      <td>Sentence: 9997</td>\n",
       "      <td>[Following, Iran, 's, disputed, June, 12, elec...</td>\n",
       "      <td>[O, B-geo, O, O, B-tim, I-tim, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47957</th>\n",
       "      <td>Sentence: 9998</td>\n",
       "      <td>[Since, then, ,, authorities, have, held, publ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47958</th>\n",
       "      <td>Sentence: 9999</td>\n",
       "      <td>[The, United, Nations, is, praising, the, use,...</td>\n",
       "      <td>[O, B-org, I-org, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47959 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sentence #                                               Word  \\\n",
       "0          Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1         Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
       "2        Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "3       Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
       "4      Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "...                ...                                                ...   \n",
       "47954   Sentence: 9995  [Opposition, leader, Mir, Hossein, Mousavi, ha...   \n",
       "47955   Sentence: 9996  [On, Thursday, ,, Iranian, state, media, publi...   \n",
       "47956   Sentence: 9997  [Following, Iran, 's, disputed, June, 12, elec...   \n",
       "47957   Sentence: 9998  [Since, then, ,, authorities, have, held, publ...   \n",
       "47958   Sentence: 9999  [The, United, Nations, is, praising, the, use,...   \n",
       "\n",
       "                                                     Tag  \n",
       "0      [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...  \n",
       "1      [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "2      [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...  \n",
       "3                      [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4      [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...  \n",
       "...                                                  ...  \n",
       "47954  [O, O, O, B-per, I-per, O, O, O, O, O, O, O, O...  \n",
       "47955  [O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...  \n",
       "47956  [O, B-geo, O, O, B-tim, I-tim, O, O, O, O, O, ...  \n",
       "47957  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "47958  [O, B-org, I-org, O, O, O, O, O, O, O, O, O, O...  \n",
       "\n",
       "[47959 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "ygghQAwnNKFF"
   },
   "outputs": [],
   "source": [
    "geoList = [\"Eastern\", \"Western\", \"Northern\", \"Southern\"]\n",
    "orgList = [\"Agency\", \"Federation\", \"Community\"]\n",
    "perList = [\"Woods\", \"Pearson\", \"Lee\", \"Mills\", \"Mason\", \"Andrews\", \"Frederick\", \"Christopher\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "_zSV96puJvT3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def generateRandomEntity(entityType):\n",
    "  if(entityType == 'GEO'):\n",
    "    return random.choice(geoList)\n",
    "  elif(entityType == 'ORG'):\n",
    "    return random.choice(orgList)\n",
    "  else:\n",
    "    return random.choice(perList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "9P3hfyZAFFz2"
   },
   "outputs": [],
   "source": [
    "def nerOversample(dataset):\n",
    "  last = False\n",
    "  for sentence in range(0, dataset.shape[0]):\n",
    "    # if (row == dataset.shape[0] - 1): last = True\n",
    "    # if((dataset.loc[row, 'Tag'] == 'B-geo' and last) or (dataset.loc[row, 'Tag'] == 'B-geo' and dataset.loc[row+1, 'Tag'] == 'O')):\n",
    "    #   dataset.loc[row, 'Tag'] = 'I-geo'\n",
    "    #   dataset.insert(row, [dataset.loc[row, 'Sentence #'], generateRandomEntity('GEO'), 'B-geo'])\n",
    "\n",
    "    # elif((dataset.loc[row, 'Tag'] == 'B-org' and last) or (dataset.loc[row, 'Tag'] == 'B-org' and dataset.loc[row+1, 'Tag'] == 'O')):\n",
    "    #   dataset.insert(row+1, [dataset.loc[row, 'Sentence #'], generateRandomEntity('ORG'), 'I-org'])\n",
    "    # elif((dataset.loc[row, 'Tag'] == 'B-per' and last) or (dataset.loc[row, 'Tag'] == 'B-per' and dataset.loc[row+1, 'Tag'] == 'O')):\n",
    "    #   dataset.insert(row+1, [dataset.loc[row, 'Sentence #'], generateRandomEntity('PER'), 'I-per'])\n",
    "\n",
    "    # sentence = 10002\n",
    "    for tag in range(0, len(dataset.loc[sentence, 'Tag'])):\n",
    "      if (tag == len(dataset.loc[sentence, 'Tag']) - 1): last = True\n",
    "      if((dataset.loc[sentence, 'Tag'][tag] == 'B-geo' and last == True) or (dataset.loc[sentence, 'Tag'][tag] == 'B-geo' and dataset.loc[sentence, 'Tag'][tag+1] == 'O')):\n",
    "        dataset.loc[sentence, 'Tag'][tag] = 'I-geo'\n",
    "        # dataset.loc[sentence, 'Tag_idx'][tag] = tag2idx.get('I-geo')\n",
    "        dataset.loc[sentence, 'Word'].insert(tag, generateRandomEntity('GEO'))\n",
    "        dataset.loc[sentence, 'Tag'].insert(tag, 'B-geo')\n",
    "        # dataset.loc[sentence, 'Tag_idx'].insert(tag, tag2idx.get('B-geo'))\n",
    "      elif((dataset.loc[sentence, 'Tag'][tag] == 'B-org' and last == True) or (dataset.loc[sentence, 'Tag'][tag] == 'B-org' and dataset.loc[sentence, 'Tag'][tag+1] == 'O')):\n",
    "        dataset.loc[sentence, 'Word'].insert(tag+1, generateRandomEntity('ORG'))\n",
    "        dataset.loc[sentence, 'Tag'].insert(tag+1, 'I-org')\n",
    "        # dataset.loc[sentence, 'Tag_idx'].insert(tag+1, tag2idx.get('I-org'))\n",
    "      elif((dataset.loc[sentence, 'Tag'][tag] == 'B-per' and last == True) or (dataset.loc[sentence, 'Tag'][tag] == 'B-per' and dataset.loc[sentence, 'Tag'][tag+1] == 'O')):\n",
    "        dataset.loc[sentence, 'Word'].insert(tag+1, generateRandomEntity('PER'))\n",
    "        dataset.loc[sentence, 'Tag'].insert(tag+1, 'I-per')\n",
    "        # dataset.loc[sentence, 'Tag_idx'].insert(tag+1, tag2idx.get('I-per'))\n",
    "      # print(dataset.loc[sentence, 'Word'][tag], dataset.loc[sentence, 'Tag'][tag], dataset.loc[sentence, 'Tag_idx'][tag])\n",
    "    #update ratio \n",
    "    # dataset.loc[sentence, 'Ratio'] = computeSentenceRatio(dataset.loc[sentence, 'Tag'])\n",
    "  return dataset\n",
    "      # print(dataset.loc[0, 'Tag'][tag])\n",
    "      # print(dataset.loc[0, 'Tag_idx'][tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "F8vxJiFShgCp"
   },
   "outputs": [],
   "source": [
    "dataset_untuk_sample = dataset_group.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "Nc5VCwDLhUUj"
   },
   "outputs": [],
   "source": [
    "dataset_sampled = nerOversample(dataset_untuk_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "JYVER4gWh8VZ",
    "outputId": "2d4ec3d5-2bf4-4e6c-a6bc-c40bfacea0fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, I-geo, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, I-geo, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[Southern, U.N., relief, coordinator, Jan, Lee...</td>\n",
       "      <td>[B-geo, I-geo, O, O, B-per, I-per, I-per, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47954</th>\n",
       "      <td>Sentence: 9995</td>\n",
       "      <td>[Opposition, leader, Mir, Hossein, Frederick, ...</td>\n",
       "      <td>[O, O, O, B-per, I-per, I-per, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47955</th>\n",
       "      <td>Sentence: 9996</td>\n",
       "      <td>[On, Thursday, ,, Iranian, state, media, publi...</td>\n",
       "      <td>[O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47956</th>\n",
       "      <td>Sentence: 9997</td>\n",
       "      <td>[Following, Northern, Iran, 's, disputed, June...</td>\n",
       "      <td>[O, B-geo, I-geo, O, O, B-tim, I-tim, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47957</th>\n",
       "      <td>Sentence: 9998</td>\n",
       "      <td>[Since, then, ,, authorities, have, held, publ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47958</th>\n",
       "      <td>Sentence: 9999</td>\n",
       "      <td>[The, United, Federation, Nations, is, praisin...</td>\n",
       "      <td>[O, B-org, I-org, I-org, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47959 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sentence #                                               Word  \\\n",
       "0          Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1         Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
       "2        Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "3       Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
       "4      Sentence: 10000  [Southern, U.N., relief, coordinator, Jan, Lee...   \n",
       "...                ...                                                ...   \n",
       "47954   Sentence: 9995  [Opposition, leader, Mir, Hossein, Frederick, ...   \n",
       "47955   Sentence: 9996  [On, Thursday, ,, Iranian, state, media, publi...   \n",
       "47956   Sentence: 9997  [Following, Northern, Iran, 's, disputed, June...   \n",
       "47957   Sentence: 9998  [Since, then, ,, authorities, have, held, publ...   \n",
       "47958   Sentence: 9999  [The, United, Federation, Nations, is, praisin...   \n",
       "\n",
       "                                                     Tag  \n",
       "0      [O, O, O, O, O, O, B-geo, I-geo, O, O, O, O, O...  \n",
       "1      [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "2      [O, O, B-tim, O, O, O, O, O, B-geo, I-geo, O, ...  \n",
       "3                      [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4      [B-geo, I-geo, O, O, B-per, I-per, I-per, O, B...  \n",
       "...                                                  ...  \n",
       "47954  [O, O, O, B-per, I-per, I-per, O, O, O, O, O, ...  \n",
       "47955  [O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...  \n",
       "47956  [O, B-geo, I-geo, O, O, B-tim, I-tim, O, O, O,...  \n",
       "47957  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "47958  [O, B-org, I-org, I-org, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[47959 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "AAu2NzfIlRtg"
   },
   "outputs": [],
   "source": [
    "word_list = []\n",
    "tag_list = []\n",
    "sentence_list = []\n",
    "for sentence in range(0, dataset_sampled.shape[0]):\n",
    "  sentence_array = [dataset_sampled.loc[sentence, 'Sentence #']] * len(dataset_sampled.loc[sentence, 'Word'])\n",
    "  sentence_list = sentence_list + sentence_array\n",
    "  word_list = word_list + dataset_sampled.loc[sentence, 'Word']\n",
    "  tag_list = tag_list + dataset_sampled.loc[sentence, 'Tag']\n",
    "  # word_list.append(dataset_sampled.loc[sentence, 'Word'])\n",
    "  # tag_list = tag_list.append(dataset_sampled.loc[sentence, 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v77WeqBXqQnT",
    "outputId": "f12ad2dd-626d-41e8-8598-25af7e6195b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119772\n",
      "1119772\n",
      "1119772\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_list))\n",
    "print(len(word_list))\n",
    "print(len(tag_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "LlaKdqhiiRh8"
   },
   "outputs": [],
   "source": [
    "# # df_after_sampling.loc[0, \"Word\"] = dataset_sampled.\n",
    "# word = concat()\n",
    "half = len(tag_list) / 2\n",
    "df_after_sampling = pd.DataFrame({'Sentence #': sentence_list, 'Word': word_list, 'Tag': tag_list})\n",
    "df_after_sampling.loc[:half].to_csv(\"./dataset/kaggle_oversampled_2nd_1.csv\")\n",
    "df_after_sampling.loc[half:].to_csv(\"./dataset/kaggle_oversampled_2nd_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "yXMts75Nr3O1",
    "outputId": "90f0a020-1ef3-4082-88cc-d9c271c8487c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119767</th>\n",
       "      <td>Sentence: 9999</td>\n",
       "      <td>weight</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119768</th>\n",
       "      <td>Sentence: 9999</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119769</th>\n",
       "      <td>Sentence: 9999</td>\n",
       "      <td>gold</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119770</th>\n",
       "      <td>Sentence: 9999</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119771</th>\n",
       "      <td>Sentence: 9999</td>\n",
       "      <td>\"</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1119772 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Sentence #           Word Tag\n",
       "0           Sentence: 1      Thousands   O\n",
       "1           Sentence: 1             of   O\n",
       "2           Sentence: 1  demonstrators   O\n",
       "3           Sentence: 1           have   O\n",
       "4           Sentence: 1        marched   O\n",
       "...                 ...            ...  ..\n",
       "1119767  Sentence: 9999         weight   O\n",
       "1119768  Sentence: 9999             in   O\n",
       "1119769  Sentence: 9999           gold   O\n",
       "1119770  Sentence: 9999              .   O\n",
       "1119771  Sentence: 9999              \"   O\n",
       "\n",
       "[1119772 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmVX7l8kr_bL",
    "outputId": "826d452d-d6f8-4634-8b7a-b8610e43a394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8294344960839113\n"
     ]
    }
   ],
   "source": [
    "print(computeRatioAwal(df_after_sampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIb7Qz73_ubI",
    "outputId": "562a5d92-5e6e-4d19-eb80-594aa90bf7ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        887908\n",
       "I-geo     42247\n",
       "B-geo     37644\n",
       "I-org     36388\n",
       "I-per     34011\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "B-per     16990\n",
       "B-gpe     15870\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_sampling[\"Tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "ADiFtpkoIicd"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_dict_map(dataset, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(dataset['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(dataset['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "token2idx, idx2token = get_dict_map(df_after_sampling, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(df_after_sampling, 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYC3TQaDLz4d",
    "outputId": "6a1f5a2a-d21b-4b63-a233-14c7739ceb9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-org': 0, 'B-art': 1, 'B-eve': 2, 'B-per': 3, 'B-org': 4, 'B-gpe': 5, 'I-eve': 6, 'I-per': 7, 'I-art': 8, 'I-tim': 9, 'B-nat': 10, 'B-tim': 11, 'I-nat': 12, 'I-gpe': 13, 'I-geo': 14, 'B-geo': 15, 'O': 16}\n"
     ]
    }
   ],
   "source": [
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzVHRVQXMaNE",
    "outputId": "c03e53dd-8b2c-4230-9c9f-dc5aee5f3b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(tag2idx.get('B-geo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cco1WLsXIpmz",
    "outputId": "c9b6285c-0a1e-4f59-c96b-dba02919c6e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sward\\.conda\\envs\\Data Mining\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "df_after_sampling['Word_idx'] = df_after_sampling['Word'].map(token2idx)\n",
    "df_after_sampling['Tag_idx'] = df_after_sampling['Tag'].map(tag2idx)\n",
    "df_after_sampling_fillna = df_after_sampling.fillna(method='ffill', axis=0)\n",
    "# Groupby and collect columns\n",
    "df_after_sampling_group = df_after_sampling_fillna.groupby(['Sentence #'], as_index=False)['Word', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "ZvTPg33qybAX",
    "outputId": "eface04f-a42b-444b-ad61-a351bac9d33b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, I-geo, O, O, O, O, O...</td>\n",
       "      <td>[21208, 31296, 8254, 31674, 1082, 16025, 20277...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 15, 14, 16, 16, 16, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[14087, 21366, 21853, 22902, 13108, 11649, 279...</td>\n",
       "      <td>[5, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, I-geo, O, ...</td>\n",
       "      <td>[8549, 10150, 24831, 32806, 29604, 27342, 3231...</td>\n",
       "      <td>[16, 16, 11, 16, 16, 16, 16, 16, 15, 14, 16, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[30823, 6198, 8514, 30656, 32626, 15994, 24172...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[Southern, U.N., relief, coordinator, Jan, Lee...</td>\n",
       "      <td>[B-geo, I-geo, O, O, B-per, I-per, I-per, O, B...</td>\n",
       "      <td>[20277, 12102, 21431, 4, 35155, 34971, 31742, ...</td>\n",
       "      <td>[15, 14, 16, 16, 3, 7, 7, 16, 11, 16, 15, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47954</th>\n",
       "      <td>Sentence: 9995</td>\n",
       "      <td>[Opposition, leader, Mir, Hossein, Frederick, ...</td>\n",
       "      <td>[O, O, O, B-per, I-per, I-per, O, O, O, O, O, ...</td>\n",
       "      <td>[7925, 58, 31424, 13764, 20989, 2796, 9203, 31...</td>\n",
       "      <td>[16, 16, 16, 3, 7, 7, 16, 16, 16, 16, 16, 16, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47955</th>\n",
       "      <td>Sentence: 9996</td>\n",
       "      <td>[On, Thursday, ,, Iranian, state, media, publi...</td>\n",
       "      <td>[O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...</td>\n",
       "      <td>[12035, 21775, 24351, 14087, 4164, 3028, 11507...</td>\n",
       "      <td>[16, 11, 16, 5, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47956</th>\n",
       "      <td>Sentence: 9997</td>\n",
       "      <td>[Following, Northern, Iran, 's, disputed, June...</td>\n",
       "      <td>[O, B-geo, I-geo, O, O, B-tim, I-tim, O, O, O,...</td>\n",
       "      <td>[30195, 12348, 7725, 33446, 11768, 1294, 20768...</td>\n",
       "      <td>[16, 15, 14, 16, 16, 11, 9, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47957</th>\n",
       "      <td>Sentence: 9998</td>\n",
       "      <td>[Since, then, ,, authorities, have, held, publ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[4672, 26735, 24351, 5628, 31674, 28171, 21437...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47958</th>\n",
       "      <td>Sentence: 9999</td>\n",
       "      <td>[The, United, Federation, Nations, is, praisin...</td>\n",
       "      <td>[O, B-org, I-org, I-org, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[7101, 29613, 31202, 4034, 32322, 27907, 29711...</td>\n",
       "      <td>[16, 4, 0, 0, 16, 16, 16, 16, 16, 16, 16, 16, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47959 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sentence #                                               Word  \\\n",
       "0          Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1         Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
       "2        Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "3       Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
       "4      Sentence: 10000  [Southern, U.N., relief, coordinator, Jan, Lee...   \n",
       "...                ...                                                ...   \n",
       "47954   Sentence: 9995  [Opposition, leader, Mir, Hossein, Frederick, ...   \n",
       "47955   Sentence: 9996  [On, Thursday, ,, Iranian, state, media, publi...   \n",
       "47956   Sentence: 9997  [Following, Northern, Iran, 's, disputed, June...   \n",
       "47957   Sentence: 9998  [Since, then, ,, authorities, have, held, publ...   \n",
       "47958   Sentence: 9999  [The, United, Federation, Nations, is, praisin...   \n",
       "\n",
       "                                                     Tag  \\\n",
       "0      [O, O, O, O, O, O, B-geo, I-geo, O, O, O, O, O...   \n",
       "1      [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "2      [O, O, B-tim, O, O, O, O, O, B-geo, I-geo, O, ...   \n",
       "3                      [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4      [B-geo, I-geo, O, O, B-per, I-per, I-per, O, B...   \n",
       "...                                                  ...   \n",
       "47954  [O, O, O, B-per, I-per, I-per, O, O, O, O, O, ...   \n",
       "47955  [O, B-tim, O, B-gpe, O, O, O, O, O, O, O, O, B...   \n",
       "47956  [O, B-geo, I-geo, O, O, B-tim, I-tim, O, O, O,...   \n",
       "47957  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "47958  [O, B-org, I-org, I-org, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                                Word_idx  \\\n",
       "0      [21208, 31296, 8254, 31674, 1082, 16025, 20277...   \n",
       "1      [14087, 21366, 21853, 22902, 13108, 11649, 279...   \n",
       "2      [8549, 10150, 24831, 32806, 29604, 27342, 3231...   \n",
       "3      [30823, 6198, 8514, 30656, 32626, 15994, 24172...   \n",
       "4      [20277, 12102, 21431, 4, 35155, 34971, 31742, ...   \n",
       "...                                                  ...   \n",
       "47954  [7925, 58, 31424, 13764, 20989, 2796, 9203, 31...   \n",
       "47955  [12035, 21775, 24351, 14087, 4164, 3028, 11507...   \n",
       "47956  [30195, 12348, 7725, 33446, 11768, 1294, 20768...   \n",
       "47957  [4672, 26735, 24351, 5628, 31674, 28171, 21437...   \n",
       "47958  [7101, 29613, 31202, 4034, 32322, 27907, 29711...   \n",
       "\n",
       "                                                 Tag_idx  \n",
       "0      [16, 16, 16, 16, 16, 16, 15, 14, 16, 16, 16, 1...  \n",
       "1      [5, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
       "2      [16, 16, 11, 16, 16, 16, 16, 16, 15, 14, 16, 1...  \n",
       "3           [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]  \n",
       "4      [15, 14, 16, 16, 3, 7, 7, 16, 11, 16, 15, 14, ...  \n",
       "...                                                  ...  \n",
       "47954  [16, 16, 16, 3, 7, 7, 16, 16, 16, 16, 16, 16, ...  \n",
       "47955  [16, 11, 16, 5, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
       "47956  [16, 15, 14, 16, 16, 11, 9, 16, 16, 16, 16, 16...  \n",
       "47957  [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...  \n",
       "47958  [16, 4, 0, 0, 16, 16, 16, 16, 16, 16, 16, 16, ...  \n",
       "\n",
       "[47959 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_after_sampling_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "0-6DeB4DI0dq"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def get_pad_train_test_val(dataset_grouped, dataset):\n",
    "\n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(dataset['Word'].to_list())))\n",
    "    n_tag = len(list(set(dataset['Tag'].to_list())))\n",
    "\n",
    "    #Pad tokens (X var)    \n",
    "    tokens = dataset_grouped['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "\n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = dataset_grouped['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tags length:', len(train_tags),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "        '\\nval_tokens:', len(val_tokens),\n",
    "        '\\nval_tags:', len(val_tags),\n",
    "    )\n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zfgcg9yYwc-",
    "outputId": "48562b55-e5d6-47a0-e37d-4818f39b1aee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 32372 \n",
      "train_tags length: 32372 \n",
      "test_tokens length: 4796 \n",
      "test_tags: 4796 \n",
      "val_tokens: 10791 \n",
      "val_tags: 10791\n",
      "train_tags:  (32372, 109, 17) val_tags:  (10791, 109, 17) test_tags:  (4796, 109, 17)\n"
     ]
    }
   ],
   "source": [
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(df_after_sampling_group, df_after_sampling)\n",
    "train_tags = np.array(train_tags)\n",
    "val_tags = np.array(val_tags)\n",
    "test_tags = np.array(test_tags)\n",
    "print('train_tags: ',train_tags.shape,'val_tags: ',val_tags.shape,'test_tags: ',test_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuPnEznJI_sm",
    "outputId": "11638375-b47d-4abf-a8d7-e885f1c93c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3528548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "train_temp_tags = np.ravel(np.argmax(train_tags, axis=-1))\n",
    "print(len(train_temp_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rolnqkh7GL8P",
    "outputId": "07446318-288e-4d60-f4d9-7f2396e860f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8.414547657146128, 1: 736.0342094284522, 2: 1022.4711677774558, 3: 18.21674978574895, 4: 15.296753412839822, 5: 19.418247456153384, 6: 1128.0524296675192, 7: 9.08207084356452, 8: 1002.7132708155726, 9: 47.2805574165885, 10: 1526.188581314879, 11: 15.149379392659188, 12: 6486.301470588235, 13: 1647.3146591970121, 14: 7.281587337618787, 15: 8.191713910285877, 16: 0.06154951575920901}\n",
      "(32372, 109)\n",
      "(32372, 109)\n"
     ]
    }
   ],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight ='balanced', \n",
    "    classes = np.unique(train_temp_tags), \n",
    "    y = train_temp_tags\n",
    "    )\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(class_weight_dict)\n",
    "\n",
    "train_label = np.argmax(train_tags, axis=-1)\n",
    "print(train_tokens.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "nGvi5V0oJJxt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "Jyq_gb1sJNQB"
   },
   "outputs": [],
   "source": [
    "input_dim = len(list(set(df_after_sampling['Word'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in df_after_sampling_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "YD-esmi8JTid"
   },
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "    model.add((Dense(n_tags, activation=\"softmax\")))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPGriHINJWsx",
    "outputId": "5a8ee827-7c1d-4294-c267-05b0f141c556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 8.414547657146128, 1: 736.0342094284522, 2: 1022.4711677774558, 3: 18.21674978574895, 4: 15.296753412839822, 5: 19.418247456153384, 6: 1128.0524296675192, 7: 9.08207084356452, 8: 1002.7132708155726, 9: 47.2805574165885, 10: 1526.188581314879, 11: 15.149379392659188, 12: 6486.301470588235, 13: 1647.3146591970121, 14: 7.281587337618787, 15: 8.191713910285877, 16: 28.52779541057316}\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "maj_index = tag2idx['O']\n",
    "\n",
    "''' Change it More for Better Fine-Tuning '''\n",
    "\n",
    "class_weight_dict[maj_index] = 28.5277954105731576 \n",
    "print(class_weight_dict)\n",
    "sample_weights = np.ones(shape=(len(train_label), train_label.shape[-1]))\n",
    "for i in range(17):\n",
    "    sample_weights[train_label == i] = class_weight_dict.get(i)\n",
    "print(sample_weights.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iv558V4rJb7i",
    "outputId": "ad8f17eb-0d82-4df9-e97c-edbebeb68a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 109, 64)           2251072   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 109, 128)          66048     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 109, 64)           49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 109, 17)           1105      \n",
      "=================================================================\n",
      "Total params: 2,367,633\n",
      "Trainable params: 2,367,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found a sample_weight array for an input with shape (32372, 109). Timestep-wise sample weighting (use of sample_weight_mode=\"temporal\") is restricted to outputs that are at least 3D, i.e. that have a time dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1d609393f184>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnlp_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bilstm_lstm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;34m\"/content/drive/MyDrive/Colab Notebooks/ner-kk/bilstm_kaggle_sampled_3.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         steps=steps_per_epoch)\n\u001b[0m\u001b[0;32m    553\u001b[0m     (x, y, sample_weights,\n\u001b[0;32m    554\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2479\u001b[0m           \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstandardize_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m           for (ref, sw, cw, mode) in zip(y, sample_weights, class_weights,\n\u001b[1;32m-> 2481\u001b[1;33m                                          feed_sample_weight_modes)\n\u001b[0m\u001b[0;32m   2482\u001b[0m       ]\n\u001b[0;32m   2483\u001b[0m       \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2478\u001b[0m       sample_weights = [\n\u001b[0;32m   2479\u001b[0m           \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstandardize_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2480\u001b[1;33m           for (ref, sw, cw, mode) in zip(y, sample_weights, class_weights,\n\u001b[0m\u001b[0;32m   2481\u001b[0m                                          feed_sample_weight_modes)\n\u001b[0;32m   2482\u001b[0m       ]\n",
      "\u001b[1;32m~\\.conda\\envs\\Data Mining\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_weights\u001b[1;34m(y, sample_weight, class_weight, sample_weight_mode)\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       raise ValueError('Found a sample_weight array for '\n\u001b[1;32m--> 953\u001b[1;33m                        \u001b[1;34m'an input with shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    954\u001b[0m                        \u001b[1;34m'Timestep-wise sample weighting (use of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m                        \u001b[1;34m'sample_weight_mode=\"temporal\") is restricted to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found a sample_weight array for an input with shape (32372, 109). Timestep-wise sample weighting (use of sample_weight_mode=\"temporal\") is restricted to outputs that are at least 3D, i.e. that have a time dimension."
     ]
    }
   ],
   "source": [
    "nlp_model = get_bilstm_lstm_model()\n",
    "plot_model(nlp_model)\n",
    "his = nlp_model.fit(train_tokens, train_label , batch_size = 64, epochs=5, validation_split=0.2, sample_weight = sample_weights) \n",
    "tf.keras.models.save_model(nlp_model, filepath  = \"/content/drive/MyDrive/Colab Notebooks/ner-kk/bilstm_kaggle_sampled_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "DgZUHDk3Jr7t",
    "outputId": "a7a62fc6-b7ed-419f-e55e-55b0799dad2f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-82ab717f400d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/ner-kk/rnn_model_sampled_2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "nlp_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/ner-kk/rnn_model_sampled_2.h5')\n",
    "\n",
    "y_test = np.argmax(test_tags, axis=-1)\n",
    "print(test_tokens.shape,y_test.shape)\n",
    "nlp_model.evaluate(test_tokens, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM5w1shLf9zN/Wbur+kg9ni",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "FP-KK-NER-RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
